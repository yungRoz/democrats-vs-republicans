{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Part Scraping\n",
    "The first part uses the congress.gov/members webpage to get the name and political\n",
    "affiliation of each congress member. Afterwards, the names and party affiliation data is saved in a\n",
    "csv file. \n",
    "\n",
    "The second part google searches based on the csv data and saves each image in \n",
    "a desktop folder for reference during training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set PATH to the data directory where you want the data saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/path/to/datafolder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build csv file \n",
    "\n",
    "The code below makes several requests to the congress.gov website and parses the page data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNamesFromPage(soup):\n",
    "    names = [] \n",
    "    for link in soup.find_all('a'):\n",
    "        url = link.get('href')\n",
    "        if \"https://www.congress.gov/member/\" not in url: \n",
    "            continue\n",
    "        else: \n",
    "            name = link.text\n",
    "            if name not in names: \n",
    "                names.append(name)\n",
    "    return names \n",
    "            \n",
    "def getPartyFromPage(soup):\n",
    "    parties = [] \n",
    "    for span in soup.find_all('span', attrs={'class': 'result-item'}):\n",
    "        text = span.text\n",
    "        if 'Republican' in text: \n",
    "            parties.append(\"Republican\")\n",
    "        if 'Democrat' in text: \n",
    "            parties.append(\"Democrat\")\n",
    "        if 'Independent' in text: \n",
    "            parties.append(\"Independent\")\n",
    "        if 'Independent Democrat' in text: \n",
    "            parties.append(\"Independent\")\n",
    "    return parties\n",
    "\n",
    "def getCongressData():\n",
    "    fullNames = []\n",
    "    fullParties = [] \n",
    "    # opens each page, to get name and party information\n",
    "    for i in range(23): \n",
    "        url = \"https://www.congress.gov/members?page=\" + str(i+1) \n",
    "        req = urllib2.Request(url, headers={'User-Agent' : \"Magic Browser\"}) \n",
    "        con = urllib2.urlopen( req )\n",
    "        soup = BeautifulSoup(con, 'html.parser')\n",
    "        names = getNamesFromPage(soup)\n",
    "        parties = getPartyFromPage(soup)\n",
    "        ## remove duplicates \n",
    "        parties = parties[::2]\n",
    "        \n",
    "        ## check that quantity of parties matches quantity of names\n",
    "        if len(parties) == len(names): \n",
    "            fullNames.extend(names) \n",
    "            fullParties.extend(parties)\n",
    "        time.sleep(1) \n",
    "    return fullNames, fullParties \n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "names, parties = getCongressData() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page data is ready to be saved at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(f\"{PATH}/congress.csv\", 'a') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerows(zip(names, parties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and save images\n",
    "\n",
    "Note: a handleful of images will throw an error, and while I was experimenting these were just saved by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adapted from http://stackoverflow.com/questions/20716842/python-download-images-from-google-image-search\n",
    "def saveImages():  \n",
    "    for i in range(len(names)): \n",
    "        \n",
    "        name = names[i]\n",
    "        party = parties[i]\n",
    "        \n",
    "        if 'Democrat' in party: \n",
    "            saveDirectory = PATH+'/dems'\n",
    "        elif 'Republican' in party: \n",
    "            saveDirectory = PATH+'/reps'\n",
    "        elif 'Independent' in party: \n",
    "            saveDirectory = PATH+'/ind'\n",
    "        else: \n",
    "            saveDirectory = PATH+'unknown'\n",
    "        \n",
    "        if not os.path.exists(saveDirectory):\n",
    "            os.makedirs(saveDirectory)\n",
    "            \n",
    "            \n",
    "        image_type=\"Action\"\n",
    "        query=name.split()\n",
    "        fn='_'.join(query) \n",
    "        fn=party+'_'+fn\n",
    "        query='+'.join(query)\n",
    "        url=\"https://www.google.co.in/search?q=\"+query+\"&source=lnms&tbm=isch\"\n",
    "        header={'User-Agent':\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36\"}\n",
    "        soup = BeautifulSoup(urllib2.urlopen(urllib2.Request(url,headers=header)),'html.parser')\n",
    "    \n",
    "        ActualImages=[]# contains the link for Large original images, type of  image\n",
    "        for a in soup.find_all(\"div\",{\"class\":\"rg_meta\"}):\n",
    "            link , Type =json.loads(a.text)[\"ou\"]  ,json.loads(a.text)[\"ity\"]\n",
    "            ActualImages.append((link,Type))\n",
    "    \n",
    "        for i , (img , Type) in enumerate( ActualImages[0:1]):\n",
    "            try:\n",
    "                ##req = urllib2.Request(img, headers={'User-Agent' : header})\n",
    "                raw_img = urllib2.urlopen(img).read()\n",
    "                if len(Type)==0:\n",
    "                    f = open(os.path.join(saveDirectory , fn+\".jpg\"), 'wb')\n",
    "                else :\n",
    "                    f = open(os.path.join(saveDirectory , fn+\".\"+Type), 'wb')\n",
    "                    f.write(raw_img)\n",
    "                    f.close()\n",
    "            except Exception as e:\n",
    "                try: \n",
    "                    req = urllib2.Request(img, headers={'User-Agent' : \"Magic Browser\"}) \n",
    "                    raw_img = urllib2.urlopen( req ).read()\n",
    "                    if len(Type)==0:\n",
    "                        f = open(os.path.join(saveDirectory , fn+\".jpg\"), 'wb')\n",
    "                    else :\n",
    "                        f = open(os.path.join(saveDirectory , fn+\".\"+Type), 'wb')\n",
    "                        f.write(raw_img)\n",
    "                        f.close()\n",
    "                except Exception as e: \n",
    "                    print(\"could not load : \"+img)\n",
    "                    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not load : http://www.tulane.edu/~rice/tdp/pix/MaxCleland.jpg\n",
      "HTTP Error 403: Forbidden\n",
      "could not load : https://digital.content.ecu.edu/adore-djatoka/resolver?url_ver=Z39.88-2004&rft_id=http://150.216.68.252/ncgre000/00000017/00016333/00016333_ac_0001.jp2&svc_id=info:lanl-repo/svc/getRegion&svc_val_fmt=info:ofi/fmt:kev:mtx:jpeg2000&svc.format=image/jpeg&svc.level=4\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)>\n",
      "could not load : https://www.ikn.army.mil/apps/IKNWMS/IKN_Websites/USAICoE/MI%20Corps%20Hall%20of%20Fame/images/HECHT,%20CHIC%20JACOB%20(SEN).jpg\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)>\n",
      "could not load : https://history.redstone.army.mil/bios/jones_intro.jpg\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)>\n",
      "could not load : https://digital.content.ecu.edu/adore-djatoka/resolver?url_ver=Z39.88-2004&rft_id=http://150.216.68.252/ncgre000/00000024/00023354/00023354_ac_0001.jp2&svc_id=info:lanl-repo/svc/getRegion&svc_val_fmt=info:ofi/fmt:kev:mtx:jpeg2000&svc.format=image/jpeg&svc.level=4\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)>\n",
      "could not load : https://jukebox.uaf.edu/ckfinder/userfiles/images/Stevens_homepage_pict.jpg\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833)>\n",
      "could not load : https://pbs.twimg.com/profile_images/748878588896768000/sjSP8Rlt_400x400.jpg\n",
      "HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "saveImages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
